(window.webpackJsonp=window.webpackJsonp||[]).push([[6],{126:function(e,a,t){"use strict";t.d(a,"a",(function(){return p}));var i=t(0),r=t.n(i);function n(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function o(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);a&&(i=i.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,i)}return t}function s(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?o(Object(t),!0).forEach((function(a){n(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function l(e,a){if(null==e)return{};var t,i,r=function(e,a){if(null==e)return{};var t,i,r={},n=Object.keys(e);for(i=0;i<n.length;i++)t=n[i],a.indexOf(t)>=0||(r[t]=e[t]);return r}(e,a);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);for(i=0;i<n.length;i++)t=n[i],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var c=r.a.createContext({}),d=function(e){var a=r.a.useContext(c),t=a;return e&&(t="function"==typeof e?e(a):s(s({},a),e)),t},u={inlineCode:"code",wrapper:function(e){var a=e.children;return r.a.createElement(r.a.Fragment,{},a)}},m=r.a.forwardRef((function(e,a){var t=e.components,i=e.mdxType,n=e.originalType,o=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),m=d(t),p=i,b=m["".concat(o,".").concat(p)]||m[p]||u[p]||n;return t?r.a.createElement(b,s(s({ref:a},c),{},{components:t})):r.a.createElement(b,s({ref:a},c))}));function p(e,a){var t=arguments,i=a&&a.mdxType;if("string"==typeof e||i){var n=t.length,o=new Array(n);o[0]=m;var s={};for(var l in a)hasOwnProperty.call(a,l)&&(s[l]=a[l]);s.originalType=e,s.mdxType="string"==typeof e?e:i,o[1]=s;for(var c=2;c<n;c++)o[c]=t[c];return r.a.createElement.apply(null,o)}return r.a.createElement.apply(null,t)}m.displayName="MDXCreateElement"},177:function(e,a,t){"use strict";t.r(a),a.default=t.p+"assets/images/example_coasttrain_naip_remapped-09f1daf8e7ee7c4793dca716bbd2eca4.png"},178:function(e,a,t){"use strict";t.r(a),a.default=t.p+"assets/images/example_coasttrain_madeira_remapped-190d6be8c803bb524c4c501895a23778.png"},179:function(e,a,t){"use strict";t.r(a),a.default=t.p+"assets/images/example_coasttrain_s2_4class-d3da59848cbb4d6a4401ef7f64a2e35b.png"},180:function(e,a,t){"use strict";t.r(a),a.default=t.p+"assets/images/example_coasttrain_quads-a1667f5e53ec4603dedefd216b6f8649.png"},181:function(e,a,t){"use strict";t.r(a),a.default=t.p+"assets/images/agreement_stats_coasttrain_naip_s2_IOU-a67396bd4f139da96b80105077c81150.png"},182:function(e,a,t){"use strict";t.r(a),a.default=t.p+"assets/images/Merged_maps_folium_ann2-7e475c692bef029444e25c9f8832f794.png"},183:function(e,a,t){"use strict";t.r(a),a.default=t.p+"assets/images/Million_pixels_vs_percentage_doodled-19772810b3bd0a5788a925f18e02ded6.png"},184:function(e,a,t){"use strict";t.r(a),a.default=t.p+"assets/images/Label_all_million_pixels_datarecords_per_ID-aa15a8fa94e7a4812e96b0dd1af7f656.png"},73:function(e,a,t){"use strict";t.r(a),t.d(a,"frontMatter",(function(){return o})),t.d(a,"metadata",(function(){return s})),t.d(a,"toc",(function(){return l})),t.d(a,"default",(function(){return d}));var i=t(3),r=t(7),n=(t(0),t(126)),o={sidebar_position:2},s={unversionedId:"Version 1: March 2022/overview",id:"Version 1: March 2022/overview",isDocsHomePage:!1,title:"Overview",description:"While there are many potential types of imagery we could use, the Coast Train project has settled on the following types of imagery because they collectively represent a majority of use-cases and scales.",source:"@site/docs/Version 1: March 2022/overview.md",sourceDirName:"Version 1: March 2022",slug:"/Version 1: March 2022/overview",permalink:"/CoastTrain/docs/Version 1: March 2022/overview",editUrl:"https://github.com/dbuscombe-usgs/CoastTrain/edit/master/website/docs/Version 1: March 2022/overview.md",version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2},sidebar:"tutorialSidebar",previous:{title:"Coast Train: A Library of Labeled Coastal Images to Train Machine Learning Models",permalink:"/CoastTrain/docs/intro"},next:{title:"Classes",permalink:"/CoastTrain/docs/Version 1: March 2022/classes"}},l=[{value:"Geospatial Imagery",id:"geospatial-imagery",children:[]},{value:"Summary graphics",id:"summary-graphics",children:[{value:"Inter-labeler Agreement",id:"inter-labeler-agreement",children:[]},{value:"Geographic Coverage",id:"geographic-coverage",children:[]},{value:"Annotation density",id:"annotation-density",children:[]},{value:"Labeler distributions",id:"labeler-distributions",children:[]}]}],c={toc:l};function d(e){var a=e.components,o=Object(r.a)(e,["components"]);return Object(n.a)("wrapper",Object(i.a)({},c,o,{components:a,mdxType:"MDXLayout"}),Object(n.a)("p",null,"While there are many potential types of imagery we could use, the Coast Train project has settled on the following types of imagery because they collectively represent a majority of use-cases and scales."),Object(n.a)("p",null,"Version 1 contains Geospatial Imagery only. These data consist of spatial and time-series, and contain 1.2 billion labelled pixels, representing over 3.6 million hectares. We use a Human-in-the-Loop tool especially designed for rapid and reproducible Earth surface image segmentation. "),Object(n.a)("p",null,"The dataset consists of 10 data records. Each dataset is associated with a specific image type, and specific label categories. Among the sets, horizontal spatial resolutions range between 0.05m and 1m for orthomosaics, and either 10m or 15m for satellite imagery. All image sources are publicly available."),Object(n.a)("ol",null,Object(n.a)("li",{parentName:"ol"},"NAIP (aerial)"),Object(n.a)("li",{parentName:"ol"},"Sentinel-2 (satellite)"),Object(n.a)("li",{parentName:"ol"},"Landsat-8 (satellite)"),Object(n.a)("li",{parentName:"ol"},"U.S. Geological Survey (USGS) Quadrangle (aerial)"),Object(n.a)("li",{parentName:"ol"},"Unmanned Aerial Survey (UAS) -derived (aerial) orthomosaic imagery. ")),Object(n.a)("p",null,"Each data record is characterized principally by the combination of image type and class set. The study was confined to locations within the conterminous United States (CONUS), and locations related to various historical and present USGS research objectives within coastal hazards and ecosystems research were prioritized. "),Object(n.a)("p",null,"We included a set of relatively recently published sets of high-resolution orthomosaic imagery created from aerial imagery collected from following a Structure-from-Motion workflow in addition to geospatial satellite imagery data available throughout CONUS. The orthomosaics are locationally specific data collectively represent muddy, sandy, and mixed-sand-gravel beaches and barrier islands, in developed and undeveloped settings."),Object(n.a)("p",null,"The number of label categories varies between four and 12. The dataset consists of 1852 individual images, comprising 1.196 billion pixels, and representing a total of 3.63 million hectares of Earth\u2019s surface. Most image sets are composed of time-series from specific sites, ranging between two and 202 individual locations. Other imagery covers an area at one specific time. Using the labeling program Doodler49 that we created for creation of this and similar datasets50, the number of pixels annotated directly by a human labeler was just over 169 million, out of over 1192 million pixels classified in total, or just over 14 percent (Table 1, Figure 4). Each labeler performed on-the-fly quality assurance through diligent usage of the labeling tool."),Object(n.a)("h2",{id:"geospatial-imagery"},"Geospatial Imagery"),Object(n.a)("ol",null,Object(n.a)("li",{parentName:"ol"},"NAIP\nThe dataset consists of 1-m NAIP imagery. There are 493 images depicting 366 unique locations.")),Object(n.a)("p",null,"Cloudless 1-m NAIP orthomosaic imagery was collected at various times in summer between 2010 and 2018. "),Object(n.a)("p",null,"Images are either 3-band (RGB) or 1-band (near-infrared) of the same extent. For each jpg file, there is a .wld file (ESRI world file format) and a aux.xml file containing all relevant coordinate system reference information for that image."),Object(n.a)("p",null,Object(n.a)("img",{src:t(177).default})),Object(n.a)("ol",{start:2},Object(n.a)("li",{parentName:"ol"},"Orthomosaics\nThe dataset consists of 5-cm orthomosaic imagery created from low altitude (<100 meters above ground level) nadir imagery using SfM processing, with variable coverage. The large ortrhomosaics have been tiled into 1024x1024x3 pixel images in jpeg format. ")),Object(n.a)("p",null,Object(n.a)("img",{src:t(178).default})),Object(n.a)("ol",{start:3},Object(n.a)("li",{parentName:"ol"},"Satellite imagery")),Object(n.a)("p",null,"Sentinel-2 imagery was collected over the period 2017-2020, and Landsat-8 imagery over the period 2014-2020. All Landsat imagery were pan-sharpened using a method based on principal components of the 15-m panchromatic band, resulting in 3-band imagery with 15-m pixel size. Visible-band 10-m Sentinel-2 imagery was used. Visible-spectrum (blue, green, and red bands) imagery were labeled, which was necessary to identify all the various classes."),Object(n.a)("p",null,Object(n.a)("img",{src:t(179).default})),Object(n.a)("ol",{start:4},Object(n.a)("li",{parentName:"ol"},"Quads\nUSGS quadrangle imagery43 depict mud-dominated delta and wetland environments of the Mississippi delta in Louisiana, collected in summer 2008 and 2012. ")),Object(n.a)("p",null,"USGS Digital Ortho Quadrangle imagery of coastal wetlands in the Gulf. See ",Object(n.a)("a",{parentName:"p",href:"https://www.usgs.gov/faqs/what-a-digital-orthophoto-quadrangle-doq-or-orthoimage?qt-news_science_products=0#qt-news_science_products"},"here")),Object(n.a)("p",null,Object(n.a)("img",{src:t(180).default})),Object(n.a)("h2",{id:"summary-graphics"},"Summary graphics"),Object(n.a)("h3",{id:"inter-labeler-agreement"},"Inter-labeler Agreement"),Object(n.a)("p",null,"We computed mean Intersection over Union (IoU) scores for quantifying inter-labeler agreement. We use 120 images across two datasets, namely NAIP (70 image pairs) and Sentinel-2 (50 image pairs), that have been labeled independently by our most experienced labelers"),Object(n.a)("p",null,"Figure: Frequency distribution of all images labeled by mean IoU scores, for the a) NAIP-11 class and b) Sentinel-2 11-class datasets. "),Object(n.a)("p",null,Object(n.a)("img",{src:t(181).default})),Object(n.a)("h3",{id:"geographic-coverage"},"Geographic Coverage"),Object(n.a)("p",null,"Collectively, the data records have been chosen to represent a wide variety of coastal environments, collectively spanning the geographic range 26 to 48 degrees N in latitude, and 69 to 123 degrees W in longitude. The majority of coastal states are represented. The final dataset contains numerous (but unequal) examples of coasts dominated by rocky cliffs, wetlands, saltmarshes, deltas, and beaches, including rural and urban locations, and low- and high-energy environments. "),Object(n.a)("p",null,"Figure: Geographical distribution of A) orthomosaic and B) satellite imagery, and C) the \u2018heatmap\u2019 of image locations, or the number of images in spatial bins."),Object(n.a)("p",null,Object(n.a)("img",{src:t(182).default})),Object(n.a)("h3",{id:"annotation-density"},"Annotation density"),Object(n.a)("p",null,"The percentage of pixels directly annotated by a human also varies considerably among individual datasets. The percentage of pixels annotated and total pixels labeled are negatively correlated; labelers tend to annotate a larger proportion of lower-resolution scenes."),Object(n.a)("p",null,"Figure: The size of the individual datasets, expressed as millions of total pixels labeled, computed as the product of the two horizontal label image dimensions, summed over all labeled images in each set. Percentage of pixels annotated by a human is computed as the product of the two horizontal label image dimensions and the proportion of the image labeled using the labeling program \u2018Doodler\u2019, summed over all labeled images in each set. "),Object(n.a)("p",null,Object(n.a)("img",{src:t(183).default})),Object(n.a)("h3",{id:"labeler-distributions"},"Labeler distributions"),Object(n.a)("p",null,"The dataset was labeled by three main individuals (ID1, 2, and 3) and certain datasets were labeled by others (ID4 and ID5)."),Object(n.a)("p",null,"Figure: Frequency distribution of images labeled by unique labeler ID."),Object(n.a)("p",null,Object(n.a)("img",{src:t(184).default})))}d.isMDXComponent=!0}}]);